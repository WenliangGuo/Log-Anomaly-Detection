{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "import dataloader\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 575061 instances, 14415 anomaly, 560646 normal\n",
      "Train: 362288 instances, 11815 anomaly, 350473 normal\n",
      "Validation: 40254 instances, 1300 anomaly, 38954 normal\n",
      "Test: 172519 instances, 1300 anomaly, 171219 normal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_path = 'logs/HDFS/structured/HDFS.log_structured.csv'\n",
    "label_path = 'logs/HDFS/anomaly_label.csv'\n",
    "template_path = 'logs/HDFS/structured/HDFS.log_templates.csv'\n",
    "\n",
    "x_train, y_train, x_test, y_test = dataloader.load_HDFS(\n",
    "    log_file = log_path, \n",
    "    label_file = label_path, \n",
    "    template_file = template_path,\n",
    "    train_ratio=0.7,\n",
    "    save_csv=False)\n",
    "\n",
    "\n",
    "num_val = x_train.shape[0] // 10\n",
    "num_train = x_train.shape[0] - num_val\n",
    "\n",
    "x_val = x_train[:num_val]\n",
    "y_val = y_train[:num_val]\n",
    "x_train = x_train[num_val:]\n",
    "y_train = y_train[num_val:]\n",
    "\n",
    "num_test = x_test.shape[0]\n",
    "num_total = num_train + num_val + num_test \n",
    "\n",
    "num_train_pos = sum(y_train)\n",
    "num_val_pos = sum(y_val)\n",
    "num_test_pos = sum(y_val)\n",
    "num_pos = num_train_pos + num_val_pos + num_test_pos\n",
    "\n",
    "print('Total: {} instances, {} anomaly, {} normal' \\\n",
    "      .format(num_total, num_pos, num_total - num_pos))\n",
    "print('Train: {} instances, {} anomaly, {} normal' \\\n",
    "      .format(num_train, num_train_pos, num_train - num_train_pos))\n",
    "print('Validation: {} instances, {} anomaly, {} normal' \\\n",
    "      .format(num_val, num_val_pos, num_val - num_val_pos))\n",
    "print('Test: {} instances, {} anomaly, {} normal\\n' \\\n",
    "      .format(num_test, num_test_pos, num_test - num_test_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 256\n",
    "lr= 0.001\n",
    "num_epochs= 300\n",
    "val_interval = 10\n",
    "max_length = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_tensor = torch.Tensor(x_train)\n",
    "y_train_tensor = torch.Tensor(y_train).to(torch.int64)\n",
    "y_train_tensor = F.one_hot(y_train_tensor, num_classes = 2)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor,y_train_tensor.to(torch.float))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True) \n",
    "\n",
    "x_val_tensor = torch.Tensor(x_val[:200])\n",
    "y_val_tensor = torch.Tensor(y_val[:200]).to(torch.int64)\n",
    "y_val_tensor = F.one_hot(y_val_tensor, num_classes = 2)\n",
    "\n",
    "val_dataset = TensorDataset(x_val_tensor,y_val_tensor.to(torch.float))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Transformer(\n",
    "        in_dim= 1,\n",
    "        embed_dim= 64, \n",
    "        depth= 6,\n",
    "        heads= 8,\n",
    "        dim_head= 64,\n",
    "        dim_ratio= 2,\n",
    "        dropout= 0.1\n",
    "    ),\n",
    "    nn.Linear(max_length*64, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 2),\n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "model = nn.DataParallel(model) # multi-GPU\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "print('device: ', device)\n",
    "\n",
    "# Loss and optimizer\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "# Train the model\n",
    "loss_min = 99999\n",
    "model_name = 'best_model.pth'\n",
    "model_path = \"saved_models\"\n",
    "\n",
    "save_path = os.path.join(model_path,model_name)\n",
    "best_model = model\n",
    "train_loss_list = []\n",
    "val_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                                                  | 22/1416 [18:45<19:48:08, 51.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m seq \u001B[38;5;241m=\u001B[39m seq\u001B[38;5;241m.\u001B[39mclone()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, max_length, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     11\u001B[0m output \u001B[38;5;241m=\u001B[39m model(seq)\n\u001B[0;32m---> 12\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, \u001B[43mlabel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     13\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     14\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Begin training ......\")\n",
    "for epoch in range(1, num_epochs+1):  # Loop over the dataset multiple times\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    # Training\n",
    "    for step, (seq, label) in enumerate(tqdm(train_dataloader)):\n",
    "        seq = seq.clone().detach().view(-1, max_length, 1).to(device)\n",
    "        output = model(seq)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    ave_trainloss = train_loss / len(train_dataloader)\n",
    "    train_loss_list.append(ave_trainloss)\n",
    "    \n",
    "    if ave_valoss < loss_min:\n",
    "        loss_min = ave_valoss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        best_model = model\n",
    "        print(\"Model saved\")\n",
    "    \n",
    "    print('Epoch [{}/{}], train_loss: {:.14f}'.format(epoch, num_epochs, ave_trainloss))\n",
    "    \n",
    "    if epoch % val_interval == 0:\n",
    "        # Vaildating\n",
    "        with torch.no_grad():    \n",
    "            for step, (seq, label) in enumerate(val_dataloader):\n",
    "                seq = seq.clone().detach().view(-1, max_length, 1).to(device)\n",
    "                output = model(seq)\n",
    "                loss = criterion(output, label.to(device))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        ave_valoss = val_loss / len(val_dataloader)\n",
    "        val_loss_list.append(ave_valoss)\n",
    "        print('Epoch [{}/{}] val loss: {:.14f}'.format(epoch, num_epochs, ave_valoss))\n",
    "\n",
    "print(f\"Finished training, model saved in: {save_path} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(len(train_loss_list))\n",
    "plt.plot(xx, train_loss_list, label = \"Train\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"train_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(len(val_loss_list))\n",
    "plt.plot(xx, val_loss_list, label = \"Val\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"val_loss.png\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d371ee047e82898569294ec9a92bd6efded7a8553613c637f3d7c29bad530d5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
